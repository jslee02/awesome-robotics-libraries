#!/usr/bin/env python3
"""Parse lychee JSON report and fix broken links in markdown files.

Handles two categories of fixes:
  1. Redirected URLs  - updates to the final destination after following 3xx chains
  2. Dead URLs        - replaces with Wayback Machine archived versions (if available)

Produces:
  lychee/fix-summary.md   - PR body summarising all changes
  lychee/unfixed-links.md - issue body for links that still need manual attention

GitHub Actions outputs (via $GITHUB_OUTPUT):
  fixes_applied  - 'true' when at least one markdown file was modified
  has_unfixable  - 'true' when broken links remain that could not be auto-fixed
"""

from __future__ import annotations

import json
import os
import re
import sys
from pathlib import Path


def load_report(path: str) -> dict:
    with open(path) as fh:
        return json.load(fh)


def get_redirect_fixes(report: dict) -> dict[str, str]:
    """Return {original_url: final_destination_url} for every redirected link."""
    fixes: dict[str, str] = {}
    for _source, entries in report.get("redirect_map", {}).items():
        for entry in entries:
            original = entry["url"]
            chain = entry.get("status", {}).get("redirects", {}).get("redirects", [])
            if chain:
                final = chain[-1]["url"]
                if final != original:
                    fixes[original] = final
    return fixes


def get_wayback_fixes(report: dict) -> dict[str, str]:
    """Return {dead_url: wayback_url} for every link with a Wayback suggestion."""
    fixes: dict[str, str] = {}
    for _source, suggestions in report.get("suggestion_map", {}).items():
        for item in suggestions:
            original = item.get("original", "")
            suggestion = item.get("suggestion", "")
            if original and suggestion:
                fixes[original] = suggestion
    return fixes


def get_unfixable(report: dict, fixed_urls: set[str]) -> dict[str, list[dict]]:
    """Return broken links that were *not* auto-fixed, grouped by source file."""
    unfixable: dict[str, list[dict]] = {}
    for source, entries in report.get("error_map", {}).items():
        for entry in entries:
            url = entry["url"]
            if url not in fixed_urls:
                unfixable.setdefault(source, []).append(
                    {
                        "url": url,
                        "status": entry.get("status", {}).get("text", "Unknown"),
                    }
                )
    return unfixable


def _build_replacement_pattern(fixes: dict[str, str]) -> re.Pattern[str]:
    """Build a single regex that matches any old URL, longest first."""
    sorted_urls = sorted(fixes.keys(), key=len, reverse=True)
    return re.compile("|".join(re.escape(url) for url in sorted_urls))


def apply_fixes(
    fixes: dict[str, str],
    root: Path,
) -> dict[str, list[tuple[str, str]]]:
    """Replace old URLs with new ones in every markdown file under *root*.

    Uses a single-pass regex so that overlapping prefixes (e.g.
    ``http://x.com`` vs ``http://x.com/foo``) are handled correctly â€”
    each character is consumed at most once.

    Returns ``{relative_path: [(old, new), ...]}`` for every file that changed.
    """
    pattern = _build_replacement_pattern(fixes)
    changes: dict[str, list[tuple[str, str]]] = {}

    for md_file in sorted(root.rglob("*.md")):
        if any(part.startswith(".") for part in md_file.parts):
            continue
        if "lychee" in md_file.parts:
            continue

        content = md_file.read_text(encoding="utf-8")
        seen: set[str] = set()
        file_fixes: list[tuple[str, str]] = []

        def _replacer(match: re.Match[str]) -> str:
            old = match.group(0)
            new = fixes[old]
            if old not in seen:
                file_fixes.append((old, new))
                seen.add(old)
            return new

        new_content = pattern.sub(_replacer, content)

        if new_content != content:
            md_file.write_text(new_content, encoding="utf-8")
            changes[str(md_file.relative_to(root))] = file_fixes

    return changes


def _url_cell(url: str, max_len: int = 80) -> str:
    """Truncate a URL for table display while keeping it readable."""
    if len(url) <= max_len:
        return url
    return url[: max_len - 1] + "\u2026"


def generate_pr_summary(
    redirect_fixes: dict[str, str],
    wayback_fixes: dict[str, str],
    changes: dict[str, list[tuple[str, str]]],
    unfixable: dict[str, list[dict]],
) -> str:
    lines: list[str] = [
        "## Auto-Fix Broken Links\n",
        "This PR was automatically generated by the link checker workflow.",
        "**Please review the changes before merging.**\n",
        "### Summary\n",
        f"- **{len(redirect_fixes)}** redirected URL(s) updated to final destination",
        f"- **{len(wayback_fixes)}** dead URL(s) replaced with Wayback Machine archives",
        f"- **{sum(len(v) for v in changes.values())}** total replacement(s) "
        f"across **{len(changes)}** file(s)\n",
    ]

    if redirect_fixes:
        lines += [
            "### Redirected URLs\n",
            "| Old URL | New URL |",
            "|---------|---------|",
        ]
        for old, new in sorted(redirect_fixes.items()):
            lines.append(f"| {_url_cell(old)} | {_url_cell(new)} |")
        lines.append("")

    if wayback_fixes:
        lines += [
            "### Dead URLs replaced with Wayback Machine archives\n",
            "| Dead URL | Archived URL |",
            "|----------|--------------|",
        ]
        for old, new in sorted(wayback_fixes.items()):
            lines.append(f"| {_url_cell(old)} | {_url_cell(new)} |")
        lines.append("")

    if unfixable:
        n = sum(len(v) for v in unfixable.values())
        lines += [
            f"### {n} link(s) could not be auto-fixed\n",
            "These require manual attention (a separate issue has been created):\n",
        ]
        for source, entries in sorted(unfixable.items()):
            for e in entries:
                lines.append(f"- `{e['url']}` ({e['status']})")
        lines.append("")

    return "\n".join(lines)


def generate_unfixable_report(unfixable: dict[str, list[dict]]) -> str:
    lines: list[str] = [
        "## Broken Links Requiring Manual Fix\n",
        "The following links could not be automatically fixed "
        "(no redirect detected and no Wayback Machine archive available).\n",
    ]
    for source, entries in sorted(unfixable.items()):
        lines.append(f"### `{source}`\n")
        for e in entries:
            lines.append(f"- [ ] {e['url']} \u2014 {e['status']}")
        lines.append("")

    return "\n".join(lines)


def set_output(name: str, value: str) -> None:
    path = os.environ.get("GITHUB_OUTPUT")
    if path:
        with open(path, "a") as fh:
            fh.write(f"{name}={value}\n")
    print(f"  ::output:: {name}={value}")


def main() -> None:
    if len(sys.argv) != 2:
        print(f"Usage: {sys.argv[0]} <lychee-report.json>", file=sys.stderr)
        sys.exit(1)

    report = load_report(sys.argv[1])
    root = Path(".")
    out_dir = Path("lychee")
    out_dir.mkdir(exist_ok=True)

    redirect_fixes = get_redirect_fixes(report)
    wayback_fixes = get_wayback_fixes(report)

    all_fixes: dict[str, str] = {}
    all_fixes.update(wayback_fixes)
    all_fixes.update(redirect_fixes)

    print(f"Redirect fixes : {len(redirect_fixes)}")
    print(f"Wayback fixes  : {len(wayback_fixes)}")

    unfixable = get_unfixable(report, set(all_fixes.keys()))

    if not all_fixes:
        print("No auto-fixable links found.")
        set_output("fixes_applied", "false")
        if unfixable:
            (out_dir / "unfixed-links.md").write_text(
                generate_unfixable_report(unfixable)
            )
            set_output("has_unfixable", "true")
        else:
            set_output("has_unfixable", "false")
        return

    changes = apply_fixes(all_fixes, root)
    print(f"Files modified  : {len(changes)}")
    for filepath, fixes in sorted(changes.items()):
        for old, new in fixes:
            print(f"  {filepath}: {old} -> {new}")

    (out_dir / "fix-summary.md").write_text(
        generate_pr_summary(redirect_fixes, wayback_fixes, changes, unfixable)
    )
    if unfixable:
        (out_dir / "unfixed-links.md").write_text(generate_unfixable_report(unfixable))

    set_output("fixes_applied", "true" if changes else "false")
    set_output("has_unfixable", "true" if unfixable else "false")


if __name__ == "__main__":
    main()
